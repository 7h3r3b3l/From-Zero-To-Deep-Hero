"""
               Made By
               7h3r3b3l
⠀⠀⠀⠀⠀⠀⠀⠀⢀⣠⣤⡶⢶⣆⠀⠶⣇⢀⣾⡇⢠⣿⢂⣰⡾⢀⣶⡶⣶⣤⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⢀⣤⣾⣧⣂⠻⣧⡨⠻⣷⠀⣿⠮⢹⠇⢸⡿⣸⡿⠁⣿⡟⣤⡿⠁⣨⠿⣶⣄⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⣴⣿⣿⠋⠻⣷⣤⣰⠝⡐⢝⠀⡘⠀⡈⠀⠀⠃⠘⢱⢀⢿⠠⠿⣱⠟⢩⣤⣾⣿⣷⣄⠀⠀⠀⠀
⠀⠀⢀⣾⣿⣿⣏⡉⣣⣊⠙⢓⠃⡘⠈⠀⢁⠀⠆⢀⠀⠡⠂⠄⠰⠈⠀⠙⠡⣃⣉⢙⡿⠿⣿⣿⣧⠀⠀⠀
⠀⠀⣾⣿⣿⠄⠬⠭⠭⣽⠚⠀⠃⠌⠀⡄⢸⠀⡀⠀⠀⠇⠈⠠⠁⠤⠐⠈⠝⢉⡭⠗⢲⣤⣿⣿⣿⣧⠀⠀
⠀⢠⣿⣿⣿⣶⣿⠂⠶⠀⢄⢣⠁⣄⠰⠄⢈⠂⡱⢀⠀⠀⠇⠀⠀⠀⠤⠴⠐⠂⢐⡥⢭⣥⣨⣿⣿⣿⠀⠀
⠀⢸⣿⣿⢉⣙⣛⣻⠛⣛⠀⠊⠑⠄⠈⠀⠐⠀⠀⠘⠀⠀⠠⠂⠒⡉⠀⡀⠎⠤⠀⣤⣄⣽⣿⣿⣿⣿⠀⠀
⠀⢸⣿⣿⣿⠿⠿⠀⣈⣋⠱⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠁⠀⠀⠤⠁⠬⠂⣌⣛⠛⣿⣿⣿⣿⠀⠀
⠀⢸⣿⣿⣿⣶⣿⠟⢟⣭⣴⠶⣁⢫⠀⢀⠀⠀⠀⢀⡀⠀⠀⠀⠀⠀⠈⠀⣿⣿⣷⣤⣽⣿⣿⣿⣿⣿⠀⠀
⠀⢸⣿⣿⣿⣿⣿⣷⣿⣿⣇⣼⣿⡘⢰⣛⡆⠀⠐⠡⣞⡧⠀⠀⠀⣀⠘⠿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀
⠀⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⡠⠍⠀⡀⠀⠀⠉⠁⠀⠀⠀⠀⠀⠀⠀⠈⠉⠛⠿⣿⣿⣿⣿⣿⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠈⠉⠉⠉⠉⠉⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⠀⠀⠹⣿⣿⣿⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠠⠀⠀⠀⠀⣿⣿⣿⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⠀⠀⠀⠀⣿⣿⡏⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⡇⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠀⠀⠀⠀⠀⠀⢰⠀⠀⠀⠀⣿⣿⠁⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⠀⢀⣀⠀⠀⠀⠂⠀⠀⠀⡿⠃⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣄⣀⠀⠀⠀⠀⠀⠀⠀⠀⠁⠀⠀⠀⠀

Secction 2.5 - Automatic Differentiation
d2l.ai
"""

import torch
import matplotlib.pyplot as plt
import numpy as np

## A Simple function

x = torch.arange(10.0)
x.requires_grad_(True)
y = 2 * torch.dot(x,x)
y.backward()
x.grad
